{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Financial Domain N-gram Language Model Pipelinefor audio transcription CSV data"
      ],
      "metadata": {
        "id": "pEfWmGTsFyTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "hFP8OUuyF0PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL CONFIGURATION"
      ],
      "metadata": {
        "id": "JKg16--FGLcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model configuration parameters.\"\"\"\n",
        "    n: int = 3\n",
        "    smoothing: str = 'backoff'\n",
        "    alpha: float = 0.1\n",
        "    discount: float = 0.75  # For Kneser-Ney smoothing\n",
        "    min_frequency: int = 2\n",
        "    max_vocab_size: int = 50000\n",
        "    test_split: float = 0.2"
      ],
      "metadata": {
        "id": "Pq9OM2dDF-bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "VYlKFnaiGQMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FinancialDataPreprocessor:\n",
        "    \"\"\"Text preprocessor optimized for financial earnings call transcripts.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.vocab = {}\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "        self.unk_token = \"<UNK>\"\n",
        "        self.start_token = \"<START>\"\n",
        "        self.end_token = \"<END>\"\n",
        "        self.pad_token = \"<PAD>\"\n",
        "\n",
        "        # Pre-compile regex patterns for faster processing\n",
        "        self._currency_pattern = re.compile(r'[\\$â‚¬][\\d,.]+(?:\\.\\d+)?[MBK]?\\b')\n",
        "        self._percent_pattern = re.compile(r'\\b[\\d,.]+(?:\\.\\d+)?%\\b')\n",
        "        self._year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "        self._number_pattern = re.compile(r'\\b[\\d,.]+\\b')\n",
        "        self._url_pattern = re.compile(r'http\\S+|www\\S+')\n",
        "\n",
        "        # Common financial abbreviations and terms\n",
        "        self._financial_terms = {\n",
        "            'ebit': 'EBIT',\n",
        "            'eps': 'EPS',\n",
        "            'roa': 'ROA',\n",
        "            'roe': 'ROE',\n",
        "            'gdp': 'GDP',\n",
        "            'npl': 'NPL',\n",
        "            'ncl': 'NCL',\n",
        "            'costco': '<COMPANY>',\n",
        "            'citi': '<COMPANY>',\n",
        "            'brexit': '<EVENT>',\n",
        "        }\n",
        "\n",
        "    def _normalize_financial_text(self, text: str) -> str:\n",
        "        \"\"\"Normalize financial text with domain-specific processing.\"\"\"\n",
        "        # Store original for certain patterns\n",
        "        original_text = text\n",
        "\n",
        "        # Convert to lowercase for processing\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replace financial terms\n",
        "        for term, replacement in self._financial_terms.items():\n",
        "            text = re.sub(rf'\\b{term}\\b', replacement, text)\n",
        "\n",
        "        # Handle currency amounts (keep the pattern but normalize)\n",
        "        text = self._currency_pattern.sub('<CURRENCY>', text)\n",
        "\n",
        "        # Handle percentages\n",
        "        text = self._percent_pattern.sub('<PERCENT>', text)\n",
        "\n",
        "        # Handle years\n",
        "        text = self._year_pattern.sub('<YEAR>', text)\n",
        "\n",
        "        # Handle large numbers (but keep small counts)\n",
        "        text = self._number_pattern.sub(self._normalize_number, text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _normalize_number(self, match):\n",
        "        \"\"\"Normalize numbers based on size.\"\"\"\n",
        "        num_str = match.group(0).replace(',', '')\n",
        "\n",
        "        try:\n",
        "            # Try to parse as float\n",
        "            num = float(num_str)\n",
        "\n",
        "            # Normalize very large numbers\n",
        "            if num >= 1000000:  # Millions\n",
        "                return '<LARGE_NUMBER>'\n",
        "            elif num >= 1000:  # Thousands\n",
        "                return '<MEDIUM_NUMBER>'\n",
        "            else:\n",
        "                return num_str  # Keep small numbers as is\n",
        "        except:\n",
        "            return '<NUMBER>'\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text with financial domain awareness.\"\"\"\n",
        "        normalized = self._normalize_financial_text(text)\n",
        "\n",
        "        # Simple whitespace tokenization (preserving special tokens)\n",
        "        tokens = normalized.split()\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def build_vocabulary(self, texts: List[str]) -> Counter:\n",
        "        \"\"\"Build vocabulary from financial transcripts.\"\"\"\n",
        "        print(\"Building vocabulary from financial transcripts...\")\n",
        "\n",
        "        all_tokens = []\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = self.tokenize(text)\n",
        "            all_tokens.extend(tokens)\n",
        "\n",
        "        word_counts = Counter(all_tokens)\n",
        "\n",
        "        print(f\"Raw vocabulary size: {len(word_counts)}\")\n",
        "\n",
        "        # Filter vocabulary\n",
        "        filtered_words = [\n",
        "            word for word, count in word_counts.most_common(self.config.max_vocab_size)\n",
        "            if count >= self.config.min_frequency\n",
        "        ]\n",
        "\n",
        "        # Build mappings\n",
        "        special_tokens = [self.pad_token, self.unk_token, self.start_token, self.end_token]\n",
        "        self.word_to_id = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "\n",
        "        for idx, word in enumerate(filtered_words, start=len(special_tokens)):\n",
        "            self.word_to_id[word] = idx\n",
        "\n",
        "        self.id_to_word = {idx: word for word, idx in self.word_to_id.items()}\n",
        "\n",
        "        print(f\"Final vocabulary size: {len(self.word_to_id)}\")\n",
        "        print(f\"Top 15 financial terms: {word_counts.most_common(15)}\")\n",
        "\n",
        "        return word_counts\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"Encode text to token IDs.\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        unk_id = self.word_to_id.get(self.unk_token, 1)\n",
        "        ids = [self.word_to_id.get(token, unk_id) for token in tokens]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        \"\"\"Decode token IDs back to text.\"\"\"\n",
        "        tokens = [self.id_to_word.get(idx, self.unk_token) for idx in ids]\n",
        "        return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "JvJTGaD6GaJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-GRAM LANGUAGE MODEL"
      ],
      "metadata": {
        "id": "GWG17xG_Ga_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramLanguageModel:\n",
        "    \"\"\"N-gram language model for financial text prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.n = config.n\n",
        "        self.smoothing = config.smoothing\n",
        "        self.alpha = config.alpha\n",
        "        self.discount = config.discount\n",
        "\n",
        "        self.ngram_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.context_counts = defaultdict(int)\n",
        "        self.vocab_size = 0\n",
        "        self._prob_cache = {}\n",
        "\n",
        "    def train(self, texts: List[List[int]], vocab_size: int) -> None:\n",
        "        \"\"\"Train n-gram model on financial transcripts.\"\"\"\n",
        "        print(f\"Training {self.n}-gram model with {self.smoothing} smoothing...\")\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        total_ngrams = 0\n",
        "\n",
        "        for token_ids in texts:\n",
        "            # Skip very short sequences\n",
        "            if len(token_ids) < 2:\n",
        "                continue\n",
        "\n",
        "            # Add padding for start and end\n",
        "            padded = [0] * (self.n - 1) + token_ids + [3]  # 0=PAD, 3=END\n",
        "\n",
        "            # Extract n-grams\n",
        "            for i in range(len(padded) - self.n + 1):\n",
        "                ngram = tuple(padded[i:i + self.n])\n",
        "                context = ngram[:-1]\n",
        "                target = ngram[-1]\n",
        "\n",
        "                self.ngram_counts[context][target] += 1\n",
        "                self.context_counts[context] += 1\n",
        "                total_ngrams += 1\n",
        "\n",
        "        print(f\"Total n-grams extracted: {total_ngrams:,}\")\n",
        "        print(f\"Unique contexts: {len(self.ngram_counts):,}\")\n",
        "\n",
        "    def _get_probability(self, token: int, context: Tuple) -> float:\n",
        "        \"\"\"Get probability with specified smoothing.\"\"\"\n",
        "        cache_key = (token, context)\n",
        "        if cache_key in self._prob_cache:\n",
        "            return self._prob_cache[cache_key]\n",
        "\n",
        "        if self.smoothing == 'laplace':\n",
        "            count = self.ngram_counts.get(context, {}).get(token, 0)\n",
        "            total = self.context_counts.get(context, 0)\n",
        "            prob = (count + self.alpha) / (total + self.alpha * self.vocab_size)\n",
        "\n",
        "        elif self.smoothing == 'backoff':\n",
        "            count = self.ngram_counts.get(context, {}).get(token, 0)\n",
        "            total = self.context_counts.get(context, 0)\n",
        "\n",
        "            if count > 0 and total > 0:\n",
        "                prob = count / total\n",
        "            else:\n",
        "                # Backoff to lower-order n-gram\n",
        "                if len(context) > 0:\n",
        "                    lower_context = context[1:] if len(context) > 1 else ()\n",
        "                    lower_count = self.ngram_counts.get(lower_context, {}).get(token, 0)\n",
        "                    lower_total = self.context_counts.get(lower_context, 0)\n",
        "\n",
        "                    if lower_total > 0:\n",
        "                        prob = lower_count / lower_total\n",
        "                    else:\n",
        "                        # Ultimate backoff to uniform\n",
        "                        prob = 1.0 / self.vocab_size\n",
        "                else:\n",
        "                    prob = 1.0 / self.vocab_size\n",
        "\n",
        "        elif self.smoothing == 'kneser_ney':\n",
        "            count = self.ngram_counts.get(context, {}).get(token, 0)\n",
        "            total = self.context_counts.get(context, 0)\n",
        "\n",
        "            if total > 0:\n",
        "                # Calculate discount\n",
        "                d = self.discount\n",
        "                unique_followers = len(self.ngram_counts.get(context, {}))\n",
        "\n",
        "                if count > 0:\n",
        "                    prob = max(count - d, 0) / total\n",
        "                    lambda_factor = d * unique_followers / total\n",
        "                else:\n",
        "                    prob = 0\n",
        "                    lambda_factor = d * unique_followers / total\n",
        "\n",
        "                # Backoff probability\n",
        "                if len(context) > 0:\n",
        "                    lower_context = context[1:] if len(context) > 1 else ()\n",
        "                    continuation_count = sum(1 for ctx in self.ngram_counts\n",
        "                                           if token in self.ngram_counts[ctx])\n",
        "                    total_continuations = len(self.ngram_counts)\n",
        "\n",
        "                    if total_continuations > 0:\n",
        "                        backoff_prob = continuation_count / total_continuations\n",
        "                    else:\n",
        "                        backoff_prob = 1.0 / self.vocab_size\n",
        "                else:\n",
        "                    backoff_prob = 1.0 / self.vocab_size\n",
        "\n",
        "                prob = prob + lambda_factor * backoff_prob\n",
        "            else:\n",
        "                prob = 1.0 / self.vocab_size\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown smoothing: {self.smoothing}\")\n",
        "\n",
        "        prob = max(prob, 1e-10)\n",
        "        self._prob_cache[cache_key] = prob\n",
        "        return prob\n",
        "\n",
        "    def evaluate(self, texts: List[List[int]]) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate model on test data.\"\"\"\n",
        "        print(f\"Evaluating on {len(texts)} transcripts...\")\n",
        "\n",
        "        total_log_prob = 0.0\n",
        "        total_tokens = 0\n",
        "        novel_ngrams = 0\n",
        "        total_ngrams = 0\n",
        "\n",
        "        for token_ids in texts:\n",
        "            if len(token_ids) < self.n:\n",
        "                continue\n",
        "\n",
        "            # Add padding\n",
        "            padded = [0] * (self.n - 1) + token_ids + [3]\n",
        "\n",
        "            for i in range(len(padded) - self.n + 1):\n",
        "                ngram = tuple(padded[i:i + self.n])\n",
        "                context = ngram[:-1]\n",
        "                target = ngram[-1]\n",
        "\n",
        "                # Check for novel n-grams\n",
        "                if target not in self.ngram_counts.get(context, {}):\n",
        "                    novel_ngrams += 1\n",
        "\n",
        "                prob = self._get_probability(target, context)\n",
        "                total_log_prob += np.log(prob)\n",
        "                total_tokens += 1\n",
        "                total_ngrams += 1\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_log_prob = total_log_prob / total_tokens if total_tokens > 0 else 0\n",
        "        perplexity = np.exp(-avg_log_prob) if total_tokens > 0 else float('inf')\n",
        "        novel_rate = novel_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
        "\n",
        "        metrics = {\n",
        "            'perplexity': perplexity,\n",
        "            'avg_log_prob': avg_log_prob,\n",
        "            'loss': -avg_log_prob,\n",
        "            'total_tokens': total_tokens,\n",
        "            'novel_ngrams': novel_ngrams,\n",
        "            'novel_ngram_rate': novel_rate\n",
        "        }\n",
        "\n",
        "        print(f\"  Perplexity: {perplexity:.2f}\")\n",
        "        print(f\"  Novel n-gram rate: {novel_rate:.4f} ({novel_ngrams}/{total_ngrams})\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def predict_next_words(self, context_text: str, preprocessor, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Predict next possible words given context.\"\"\"\n",
        "        # Encode context\n",
        "        context_ids = preprocessor.encode(context_text)\n",
        "\n",
        "        # Get last n-1 tokens for context\n",
        "        if len(context_ids) >= (self.n - 1):\n",
        "            context = tuple(context_ids[-(self.n - 1):])\n",
        "        else:\n",
        "            context = tuple(context_ids)\n",
        "\n",
        "        # Get probabilities for all tokens\n",
        "        predictions = []\n",
        "        for token_id in range(self.vocab_size):\n",
        "            # Skip special tokens for prediction\n",
        "            if token_id not in [0, 2, 3]:  # Not PAD, START, or END\n",
        "                prob = self._get_probability(token_id, context)\n",
        "                word = preprocessor.id_to_word.get(token_id, preprocessor.unk_token)\n",
        "                predictions.append((word, prob))\n",
        "\n",
        "        # Sort and return top k\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "        return predictions[:top_k]\n",
        "\n",
        "    def generate(self, preprocessor, max_length: int = 50,\n",
        "                 temperature: float = 1.0) -> str:\n",
        "        \"\"\"Generate financial text.\"\"\"\n",
        "        start_id = preprocessor.word_to_id.get(preprocessor.start_token, 2)\n",
        "\n",
        "        sequence = [start_id]\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Get context\n",
        "            if len(sequence) >= (self.n - 1):\n",
        "                context = tuple(sequence[-(self.n - 1):])\n",
        "            else:\n",
        "                context = tuple(sequence)\n",
        "\n",
        "            # Get probabilities for all tokens\n",
        "            probs = np.zeros(self.vocab_size)\n",
        "            for token_id in range(self.vocab_size):\n",
        "                probs[token_id] = self._get_probability(token_id, context)\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                probs = np.power(probs, 1.0 / temperature)\n",
        "\n",
        "            # Normalize and sample\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            # Avoid sampling special tokens\n",
        "            mask = np.ones(self.vocab_size, dtype=bool)\n",
        "            mask[0] = mask[2] = False  # Don't sample PAD or START\n",
        "\n",
        "            # Apply mask and renormalize\n",
        "            masked_probs = probs * mask\n",
        "            if masked_probs.sum() > 0:\n",
        "                masked_probs = masked_probs / masked_probs.sum()\n",
        "                next_token = np.random.choice(self.vocab_size, p=masked_probs)\n",
        "            else:\n",
        "                next_token = 3  # END token\n",
        "\n",
        "            sequence.append(next_token)\n",
        "\n",
        "            if next_token == 3:  # END token\n",
        "                break\n",
        "\n",
        "        # Decode, skipping the start token\n",
        "        tokens = [preprocessor.id_to_word.get(idx, preprocessor.unk_token)\n",
        "                 for idx in sequence[1:]]\n",
        "        return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "wF3mtY9XGkvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PIPELINE MANAGER"
      ],
      "metadata": {
        "id": "T_PyJzxIGlzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaM0OhfuFoXn"
      },
      "outputs": [],
      "source": [
        "class PipelineManager:\n",
        "    \"\"\"Complete pipeline for financial n-gram models.\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: str = \"./financial_ngram_results\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.config = ModelConfig()\n",
        "        self.preprocessor = None\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def load_financial_data(self, filepath: str) -> List[str]:\n",
        "        \"\"\"Load financial transcript data from CSV.\"\"\"\n",
        "        print(f\"Loading data from {filepath}...\")\n",
        "\n",
        "        try:\n",
        "            # Try to load as CSV\n",
        "            df = pd.read_csv(filepath, sep='|', header=0)\n",
        "\n",
        "            # Check if transcript column exists\n",
        "            if 'transcript' in df.columns:\n",
        "                texts = df['transcript'].dropna().tolist()\n",
        "            elif len(df.columns) >= 3:  # Assume 3rd column is transcript\n",
        "                texts = df.iloc[:, 2].dropna().tolist()\n",
        "            else:\n",
        "                raise ValueError(\"Could not find transcript column\")\n",
        "\n",
        "            print(f\"Loaded {len(texts)} financial transcripts\")\n",
        "\n",
        "            # Show sample\n",
        "            print(\"\\nSample transcripts:\")\n",
        "            for i in range(min(3, len(texts))):\n",
        "                print(f\"  {i+1}. {texts[i][:80]}...\")\n",
        "\n",
        "            return texts\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading CSV: {e}\")\n",
        "            print(\"Trying alternative loading method...\")\n",
        "\n",
        "            # Fallback: read as text file\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            # Skip header if exists\n",
        "            if 'transcript' in lines[0].lower():\n",
        "                lines = lines[1:]\n",
        "\n",
        "            # Extract transcripts (assuming format: ...|transcript)\n",
        "            texts = []\n",
        "            for line in lines:\n",
        "                parts = line.strip().split('|')\n",
        "                if len(parts) >= 3:\n",
        "                    texts.append(parts[-1])\n",
        "\n",
        "            print(f\"Loaded {len(texts)} transcripts (fallback method)\")\n",
        "            return texts\n",
        "\n",
        "    def run_pipeline(self, data_file: str) -> Dict:\n",
        "        \"\"\"Run complete pipeline on financial data.\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"FINANCIAL EARNINGS CALL N-GRAM PIPELINE\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Step 1: Load and preprocess data\n",
        "        print(\"\\n[1] Loading Financial Data\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        texts = self.load_financial_data(data_file)\n",
        "\n",
        "        if not texts:\n",
        "            print(\"No data loaded. Exiting.\")\n",
        "            return {}\n",
        "\n",
        "        # Step 2: Preprocessing\n",
        "        print(\"\\n[2] Preprocessing Financial Text\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        self.preprocessor = FinancialDataPreprocessor(self.config)\n",
        "        word_counts = self.preprocessor.build_vocabulary(texts)\n",
        "\n",
        "        # Save vocabulary stats\n",
        "        self._save_vocab_stats(word_counts)\n",
        "\n",
        "        # Encode all texts\n",
        "        print(\"Encoding transcripts...\")\n",
        "        encoded_texts = [self.preprocessor.encode(text) for text in texts]\n",
        "\n",
        "        # Filter out empty sequences\n",
        "        encoded_texts = [seq for seq in encoded_texts if len(seq) > 0]\n",
        "\n",
        "        print(f\"Total encoded sequences: {len(encoded_texts)}\")\n",
        "        print(f\"Average sequence length: {np.mean([len(seq) for seq in encoded_texts]):.1f}\")\n",
        "\n",
        "        # Train/test split\n",
        "        split_idx = int(len(encoded_texts) * (1 - self.config.test_split))\n",
        "        train_texts = encoded_texts[:split_idx]\n",
        "        test_texts = encoded_texts[split_idx:]\n",
        "\n",
        "        print(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")\n",
        "\n",
        "        # Step 3: Train and evaluate different models\n",
        "        print(\"\\n[3] Training and Evaluating Models\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        self.results['comparisons'] = {}\n",
        "\n",
        "        # Test different configurations\n",
        "        configurations = [\n",
        "            (2, 'backoff'),\n",
        "            (2, 'laplace'),\n",
        "            (3, 'backoff'),\n",
        "            (3, 'kneser_ney'),\n",
        "        ]\n",
        "\n",
        "        best_perplexity = float('inf')\n",
        "        best_model_info = None\n",
        "\n",
        "        for n, smoothing in configurations:\n",
        "            config = ModelConfig(\n",
        "                n=n,\n",
        "                smoothing=smoothing,\n",
        "                alpha=self.config.alpha,\n",
        "                discount=self.config.discount,\n",
        "                min_frequency=self.config.min_frequency,\n",
        "                max_vocab_size=self.config.max_vocab_size\n",
        "            )\n",
        "\n",
        "            model_key = f\"{n}-gram_{smoothing}\"\n",
        "            print(f\"\\nTraining {model_key}...\")\n",
        "\n",
        "            model = NGramLanguageModel(config)\n",
        "            model.train(train_texts, len(self.preprocessor.word_to_id))\n",
        "\n",
        "            # Evaluate\n",
        "            print(f\"Evaluating {model_key}...\")\n",
        "            metrics = model.evaluate(test_texts)\n",
        "\n",
        "            self.results['comparisons'][model_key] = {\n",
        "                'model': model,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "\n",
        "            # Track best model\n",
        "            if metrics['perplexity'] < best_perplexity:\n",
        "                best_perplexity = metrics['perplexity']\n",
        "                best_model_info = (model_key, model, metrics)\n",
        "\n",
        "        # Step 4: Detailed analysis with best model\n",
        "        print(\"\\n[4] Detailed Analysis with Best Model\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        if best_model_info:\n",
        "            best_key, best_model, best_metrics = best_model_info\n",
        "            self.results['best_model'] = {\n",
        "                'key': best_key,\n",
        "                'model': best_model,\n",
        "                'metrics': best_metrics\n",
        "            }\n",
        "\n",
        "            print(f\"\\nBest Model: {best_key}\")\n",
        "            print(f\"Perplexity: {best_metrics['perplexity']:.2f}\")\n",
        "            print(f\"Novel n-gram rate: {best_metrics['novel_ngram_rate']:.4f}\")\n",
        "\n",
        "            # Generate samples\n",
        "            print(\"\\nGenerating financial text samples:\")\n",
        "            for i in range(3):\n",
        "                generated = best_model.generate(self.preprocessor, max_length=30, temperature=0.8)\n",
        "                print(f\"  Sample {i+1}: {generated}\")\n",
        "\n",
        "            # Show predictions on actual context\n",
        "            print(\"\\nPredicting next words from actual context:\")\n",
        "            sample_contexts = [\n",
        "                \"revenue increased by\",\n",
        "                \"earnings per share\",\n",
        "                \"we are continuing to invest\",\n",
        "            ]\n",
        "\n",
        "            for context in sample_contexts:\n",
        "                predictions = best_model.predict_next_words(context, self.preprocessor, top_k=3)\n",
        "                print(f\"\\n  Context: '{context}'\")\n",
        "                print(f\"  Top predictions:\")\n",
        "                for word, prob in predictions:\n",
        "                    print(f\"    - {word}: {prob:.4f}\")\n",
        "\n",
        "        # Step 5: Create visualizations\n",
        "        print(\"\\n[5] Creating Visualizations\")\n",
        "        print(\"-\" * 80)\n",
        "        self._create_visualizations()\n",
        "\n",
        "        # Step 6: Save results\n",
        "        print(\"\\n[6] Saving Results\")\n",
        "        print(\"-\" * 80)\n",
        "        self._save_results()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"PIPELINE COMPLETE\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def _save_vocab_stats(self, word_counts: Counter):\n",
        "        \"\"\"Save vocabulary statistics.\"\"\"\n",
        "        stats = {\n",
        "            'vocab_size': len(self.preprocessor.word_to_id),\n",
        "            'total_unique_words': len(word_counts),\n",
        "            'top_30_financial_terms': word_counts.most_common(30),\n",
        "            'vocab_distribution': {\n",
        "                '1-5 occurrences': sum(1 for count in word_counts.values() if 1 <= count <= 5),\n",
        "                '6-20 occurrences': sum(1 for count in word_counts.values() if 6 <= count <= 20),\n",
        "                '21-100 occurrences': sum(1 for count in word_counts.values() if 21 <= count <= 100),\n",
        "                '100+ occurrences': sum(1 for count in word_counts.values() if count > 100),\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(self.output_dir / 'vocab_stats.json', 'w') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "\n",
        "        print(\"Vocabulary statistics saved\")\n",
        "\n",
        "    def _create_visualizations(self):\n",
        "        \"\"\"Create visualizations for model comparison.\"\"\"\n",
        "        if not self.results.get('comparisons'):\n",
        "            return\n",
        "\n",
        "        models = list(self.results['comparisons'].keys())\n",
        "\n",
        "        # Extract metrics\n",
        "        perplexities = []\n",
        "        novel_rates = []\n",
        "\n",
        "        for model in models:\n",
        "            metrics = self.results['comparisons'][model]['metrics']\n",
        "            perplexities.append(metrics['perplexity'])\n",
        "            novel_rates.append(metrics['novel_ngram_rate'])\n",
        "\n",
        "        # Create comparison plot\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        fig.suptitle('Financial N-Gram Model Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Perplexity comparison\n",
        "        ax = axes[0, 0]\n",
        "        colors = ['skyblue' if '2-gram' in m else 'lightcoral' for m in models]\n",
        "        bars = ax.bar(range(len(models)), perplexities, color=colors, edgecolor='black')\n",
        "        ax.set_ylabel('Perplexity', fontsize=12)\n",
        "        ax.set_title('Perplexity by Model (Lower is Better)', fontsize=13)\n",
        "        ax.set_xticks(range(len(models)))\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        for bar, val in zip(bars, perplexities):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{val:.1f}', ha='center', va='bottom')\n",
        "\n",
        "        # Novel n-gram rate\n",
        "        ax = axes[0, 1]\n",
        "        bars = ax.bar(range(len(models)), novel_rates, color=colors, edgecolor='black')\n",
        "        ax.set_ylabel('Novel N-gram Rate', fontsize=12)\n",
        "        ax.set_title('Novel N-gram Rate (Generalization)', fontsize=13)\n",
        "        ax.set_xticks(range(len(models)))\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.set_ylim([0, 1])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        for bar, val in zip(bars, novel_rates):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{val:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # Relationship between perplexity and novel rate\n",
        "        ax = axes[1, 0]\n",
        "        scatter = ax.scatter(novel_rates, perplexities, s=100, alpha=0.6, edgecolors='black')\n",
        "        ax.set_xlabel('Novel N-gram Rate', fontsize=12)\n",
        "        ax.set_ylabel('Perplexity', fontsize=12)\n",
        "        ax.set_title('Perplexity vs Novel Rate', fontsize=13)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add labels for each point\n",
        "        for i, model in enumerate(models):\n",
        "            ax.annotate(model, (novel_rates[i], perplexities[i]),\n",
        "                       xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "\n",
        "        # Summary table\n",
        "        ax = axes[1, 1]\n",
        "        ax.axis('off')\n",
        "\n",
        "        table_data = [['Model', 'Perplexity', 'Novel Rate', 'Tokens']]\n",
        "        for i, model in enumerate(models):\n",
        "            metrics = self.results['comparisons'][model]['metrics']\n",
        "            table_data.append([\n",
        "                model,\n",
        "                f\"{metrics['perplexity']:.1f}\",\n",
        "                f\"{metrics['novel_ngram_rate']:.3f}\",\n",
        "                f\"{metrics['total_tokens']:,}\"\n",
        "            ])\n",
        "\n",
        "        table = ax.table(cellText=table_data, cellLoc='center',\n",
        "                        loc='center', colWidths=[0.3, 0.2, 0.2, 0.2])\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(10)\n",
        "        table.scale(1, 1.5)\n",
        "\n",
        "        # Style table\n",
        "        for i in range(len(table_data)):\n",
        "            for j in range(4):\n",
        "                if i == 0:\n",
        "                    table[(i, j)].set_facecolor('#40466e')\n",
        "                    table[(i, j)].set_text_props(weight='bold', color='white')\n",
        "                elif i % 2 == 0:\n",
        "                    table[(i, j)].set_facecolor('#f5f5f5')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.output_dir / 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"Visualizations saved\")\n",
        "\n",
        "    def _save_results(self):\n",
        "        \"\"\"Save all pipeline results.\"\"\"\n",
        "        # Prepare results for saving\n",
        "        results_to_save = {\n",
        "            'best_model': {\n",
        "                'key': self.results.get('best_model', {}).get('key', ''),\n",
        "                'metrics': self.results.get('best_model', {}).get('metrics', {})\n",
        "            } if 'best_model' in self.results else {},\n",
        "            'comparisons': {\n",
        "                model: {'metrics': data['metrics']}\n",
        "                for model, data in self.results.get('comparisons', {}).items()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save JSON results\n",
        "        with open(self.output_dir / 'results.json', 'w') as f:\n",
        "            json.dump(results_to_save, f, indent=2)\n",
        "\n",
        "        # Save summary report\n",
        "        self._create_summary_report()\n",
        "\n",
        "        print(f\"\\nAll results saved to: {self.output_dir}\")\n",
        "\n",
        "    def _create_summary_report(self):\n",
        "        \"\"\"Create a text summary report.\"\"\"\n",
        "        report_path = self.output_dir / 'summary_report.txt'\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "            f.write(\"FINANCIAL N-GRAM LANGUAGE MODEL - SUMMARY REPORT\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "            # Best model\n",
        "            if 'best_model' in self.results:\n",
        "                best = self.results['best_model']\n",
        "                f.write(\"BEST MODEL:\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\")\n",
        "                f.write(f\"Configuration: {best['key']}\\n\\n\")\n",
        "\n",
        "                metrics = best['metrics']\n",
        "                f.write(\"Performance Metrics:\\n\")\n",
        "                f.write(f\"  Perplexity: {metrics['perplexity']:.2f}\\n\")\n",
        "                f.write(f\"  Novel n-gram rate: {metrics['novel_ngram_rate']:.4f}\\n\")\n",
        "                f.write(f\"  Total test tokens: {metrics['total_tokens']:,}\\n\")\n",
        "                f.write(f\"  Average log probability: {metrics['avg_log_prob']:.4f}\\n\\n\")\n",
        "\n",
        "            # All models comparison\n",
        "            f.write(\"ALL MODELS COMPARISON:\\n\")\n",
        "            f.write(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "            for model, data in self.results.get('comparisons', {}).items():\n",
        "                metrics = data['metrics']\n",
        "                f.write(f\"\\n{model}:\\n\")\n",
        "                f.write(f\"  Perplexity: {metrics['perplexity']:.2f}\\n\")\n",
        "                f.write(f\"  Novel rate: {metrics['novel_ngram_rate']:.4f}\\n\")\n",
        "                f.write(f\"  Test tokens: {metrics['total_tokens']:,}\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "        print(\"Summary report saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXECUTION"
      ],
      "metadata": {
        "id": "Lu7ku0bwGwS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Initialize pipeline\n",
        "    pipeline = PipelineManager(output_dir=\"./financial_ngram_results\")\n",
        "\n",
        "    # Run pipeline on your data\n",
        "    results = pipeline.run_pipeline(\n",
        "        data_file=\"/content/train.csv\"  # Update with your actual file path\n",
        "    )\n",
        "\n",
        "    # Show final summary\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"FINAL SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        if 'best_model' in results:\n",
        "            best = results['best_model']\n",
        "            print(f\"\\nBest Model: {best['key']}\")\n",
        "            print(f\"Perplexity: {best['metrics']['perplexity']:.2f}\")\n",
        "            print(f\"Novel n-gram rate: {best['metrics']['novel_ngram_rate']:.4f}\")\n",
        "\n",
        "        print(\"\\nModel Comparison:\")\n",
        "        for model, data in results.get('comparisons', {}).items():\n",
        "            metrics = data['metrics']\n",
        "            print(f\"  {model}: Perplexity={metrics['perplexity']:.2f}, \"\n",
        "                  f\"Novel={metrics['novel_ngram_rate']:.3f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "DMO5RSJ7Gvi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixRYRR1OG1-s",
        "outputId": "dd94023e-cfa4-4ed3-c432-e67fb0a99f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FINANCIAL EARNINGS CALL N-GRAM PIPELINE\n",
            "================================================================================\n",
            "\n",
            "[1] Loading Financial Data\n",
            "--------------------------------------------------------------------------------\n",
            "Loading data from /content/train.csv...\n",
            "Loaded 1261521 financial transcripts\n",
            "\n",
            "Sample transcripts:\n",
            "  1. I have to say that I'm very encouraged by the underlying momentum across our fra...\n",
            "  2. Our Fixed Income franchise continued to benefit from strong client engagement ac...\n",
            "  3. As a sign of our commitment to Mexico, a market where we have real scale and con...\n",
            "\n",
            "[2] Preprocessing Financial Text\n",
            "--------------------------------------------------------------------------------\n",
            "Building vocabulary from financial transcripts...\n",
            "Raw vocabulary size: 158794\n",
            "Final vocabulary size: 50004\n",
            "Top 15 financial terms: [('the', 1632468), ('to', 1071467), ('and', 1026343), ('of', 873625), ('we', 782604), ('in', 707860), ('that', 596183), ('our', 584325), ('a', 553396), ('is', 316647), ('as', 283880), ('for', 280686), ('on', 280509), ('are', 252250), ('have', 235850)]\n",
            "Vocabulary statistics saved\n",
            "Encoding transcripts...\n",
            "Total encoded sequences: 1261521\n",
            "Average sequence length: 24.1\n",
            "Train: 1009216, Test: 252305\n",
            "\n",
            "[3] Training and Evaluating Models\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Training 2-gram_backoff...\n",
            "Training 2-gram model with backoff smoothing...\n",
            "Total n-grams extracted: 25,311,557\n",
            "Unique contexts: 49,971\n",
            "Evaluating 2-gram_backoff...\n",
            "Evaluating on 252305 transcripts...\n",
            "  Perplexity: 132.30\n",
            "  Novel n-gram rate: 0.0601 (380481/6335402)\n",
            "\n",
            "Training 2-gram_laplace...\n",
            "Training 2-gram model with laplace smoothing...\n",
            "Total n-grams extracted: 25,311,557\n",
            "Unique contexts: 49,971\n",
            "Evaluating 2-gram_laplace...\n",
            "Evaluating on 252305 transcripts...\n",
            "  Perplexity: 226.75\n",
            "  Novel n-gram rate: 0.0601 (380481/6335402)\n",
            "\n",
            "Training 3-gram_backoff...\n",
            "Training 3-gram model with backoff smoothing...\n",
            "Total n-grams extracted: 25,311,557\n",
            "Unique contexts: 2,410,650\n",
            "Evaluating 3-gram_backoff...\n",
            "Evaluating on 252305 transcripts...\n",
            "  Perplexity: 216.25\n",
            "  Novel n-gram rate: 0.2763 (1750777/6335399)\n",
            "\n",
            "Training 3-gram_kneser_ney...\n",
            "Training 3-gram model with kneser_ney smoothing...\n",
            "Total n-grams extracted: 25,311,557\n",
            "Unique contexts: 2,410,650\n",
            "Evaluating 3-gram_kneser_ney...\n",
            "Evaluating on 252305 transcripts...\n"
          ]
        }
      ]
    }
  ]
}