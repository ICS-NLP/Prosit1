{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Akan Text Generation with Transformers\n",
        "\n",
        "This notebook trains a character-level Tokenizer and Transformer model to generate Akan text, based on transcriptions from 'Akan.xlsx'."
      ],
      "metadata": {
        "id": "qpMLUylg4czs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch pandas openpyxl tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CZI947zb4czu",
        "outputId": "4b9c3fe5-baeb-4506-ba85-cc5c28153a07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "aJYzRoSE4czu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "Optimized for the Akan dataset (~3MB text)."
      ],
      "metadata": {
        "id": "VZ04oepb4czv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64  # Increased batch size for efficiency\n",
        "block_size = 128 # Context length\n",
        "max_iters = 3000 # Reduced iterations for efficiency (approx 10 epochs)\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128     # Reduced embedding size to prevent overfitting\n",
        "n_head = 4       # Reduced heads\n",
        "n_layer = 4      # Reduced layers (lighter model)\n",
        "dropout = 0.2    # Increased dropout for regularization\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1ii5T5Q4czv",
        "outputId": "51ae1407-4fd2-45a5-f02f-37adc5a0bb1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e644bf002f0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "Loads text from the 'Transcriptions' column of Akan.xlsx."
      ],
      "metadata": {
        "id": "qMjsOiE64czv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_language_texts():\n",
        "    \"\"\"Load Akan dataset from Excel\"\"\"\n",
        "    print(\"Loading Akan dataset from Excel...\")\n",
        "    file_path = \"/content/Akan.xlsx\"\n",
        "\n",
        "    try:\n",
        "        # Try loading from current directory or absolute path if needed\n",
        "        if os.path.exists(file_path):\n",
        "             df = pd.read_excel(file_path)\n",
        "        else:\n",
        "             # Fallback to specific user path if running locally\n",
        "             base_path = \"/Users/naalamleboye/Documents/Ashesi_MPhil_ICS/2025-2026_Sem2/Natural Language Processing/Prosit 1/\"\n",
        "             df = pd.read_excel(os.path.join(base_path, file_path))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading Excel file: {e}\")\n",
        "        print(\"Ensure 'Akan.xlsx' is in the directory and openpyxl is installed.\")\n",
        "        raise\n",
        "\n",
        "    if 'Transcriptions' not in df.columns:\n",
        "        raise ValueError(f\"Column 'Transcriptions' not found. Available: {df.columns.tolist()}\")\n",
        "\n",
        "    # Drop NA and convert to string\n",
        "    texts = df['Transcriptions'].dropna().astype(str).tolist()\n",
        "    print(f\"Collected {len(texts)} texts.\")\n",
        "\n",
        "    # Combine all texts with newlines\n",
        "    combined_text = '\\n\\n'.join(texts)\n",
        "    return combined_text\n",
        "\n",
        "def create_vocab_and_encode(text):\n",
        "    \"\"\"Create vocabulary and encoding/decoding functions\"\"\"\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    print(f\"Vocabulary size: {vocab_size} unique characters\")\n",
        "\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "    return vocab_size, encode, decode, stoi, itos\n",
        "\n",
        "def prepare_data(text, encode):\n",
        "    \"\"\"Prepare train/val/test splits\"\"\"\n",
        "    print(\"Encoding text...\")\n",
        "    data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "    n = len(data)\n",
        "    train_size = int(0.9 * n) # 90% train\n",
        "    val_size = int(0.05 * n)  # 5% val\n",
        "    test_size = n - train_size - val_size\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:train_size + val_size]\n",
        "    test_data = data[train_size + val_size:]\n",
        "\n",
        "    print(f\"Train size: {len(train_data):,} chars\")\n",
        "    print(f\"Val size:   {len(val_data):,} chars\")\n",
        "    print(f\"Test size:  {len(test_data):,} chars\")\n",
        "\n",
        "    return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "mm_XDaST4czv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "Standard Decoder-only Transformer."
      ],
      "metadata": {
        "id": "-S1Kr0tQ4czv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"One head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,head_size)\n",
        "        q = self.query(x) # (B,T,head_size)\n",
        "\n",
        "        # Affinity scores\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "zq3EnsnJ4czw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Generation Utilities"
      ],
      "metadata": {
        "id": "jnGIJ9WM4czw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size, block_size):\n",
        "    \"\"\"Generate a small batch of data\"\"\"\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_data, val_data):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split, data in [('train', train_data), ('val', val_data)]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(data, batch_size, block_size)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def train_model(model, train_data, val_data, optimizer):\n",
        "    print(\"Starting training...\")\n",
        "    best_val_loss = float('inf')\n",
        "    start_time = time.time()\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss(model, train_data, val_data)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "            if losses['val'] < best_val_loss:\n",
        "                best_val_loss = losses['val']\n",
        "                torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        xb, yb = get_batch(train_data, batch_size, block_size)\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Training finished in {(time.time()-start_time)/60:.2f} minutes.\")\n",
        "\n",
        "def generate_text(model, decode, start_string, max_new_tokens=200, temperature=0.8):\n",
        "    model.eval()\n",
        "    # Handle unknown chars by ignoring or mapping to random known char (simple approach: skip)\n",
        "    # But for now assume start_string chars exist in vocab\n",
        "    try:\n",
        "        context = torch.tensor([encode(start_string)], dtype=torch.long, device=device)\n",
        "        generated = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "        return decode(generated[0].tolist())\n",
        "    except KeyError as e:\n",
        "        return f\"Error: Character {e} not in vocabulary.\"\n"
      ],
      "metadata": {
        "id": "UJ2jKNF74czw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Execution"
      ],
      "metadata": {
        "id": "D2xAP-Yk4czw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup, Data Loading & Model Initialization"
      ],
      "metadata": {
        "id": "c1mqCVkS4czw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "text = load_language_texts()\n",
        "\n",
        "# 2. Vocab\n",
        "vocab_size, encode, decode, stoi, itos = create_vocab_and_encode(text)\n",
        "\n",
        "# 3. Splits\n",
        "train_data, val_data, test_data = prepare_data(text, encode)\n",
        "\n",
        "# 4. Initialize Model\n",
        "print(f\"Initializing model with embedding size {n_embd}, {n_layer} layers, {n_head} heads.\")\n",
        "model = TransformerLanguageModel(vocab_size)\n",
        "model = model.to(device)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp7JFH394czw",
        "outputId": "3a333ee3-6b52-456b-c89f-1454eea256ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Akan dataset from Excel...\n",
            "Collected 18787 texts.\n",
            "Vocabulary size: 90 unique characters\n",
            "Encoding text...\n",
            "Train size: 2,925,977 chars\n",
            "Val size:   162,554 chars\n",
            "Test size:  162,555 chars\n",
            "Initializing model with embedding size 128, 4 layers, 4 heads.\n",
            "Parameters: 0.83M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Evaluation Metric\n",
        "We evaluate language models using cross-entropy loss and perplexity.\n",
        "- **Loss**: Measures how well the model predicts the next character (lower is better).\n",
        "- **Perplexity (exp(loss))**: A more intuitive metric representing the geometric mean of the number of choices the model believes are possible for the next character. A lower perplexity indicates the model is more confident and accurate."
      ],
      "metadata": {
        "id": "nIjuXv1D4czw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, data, subset_name=\"test\"):\n",
        "    \"\"\"Evaluate the model on a given dataset and print Loss and Perplexity\"\"\"\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(data, batch_size, block_size)\n",
        "        logits, loss = model(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "\n",
        "    mean_loss = losses.mean().item()\n",
        "    perplexity = torch.exp(torch.tensor(mean_loss)).item()\n",
        "\n",
        "    print(f\"--- {subset_name.upper()} SET EVALUATION ---\")\n",
        "    print(f\"Loss:       {mean_loss:.4f}\")\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "    return mean_loss, perplexity"
      ],
      "metadata": {
        "id": "a6v-jO7P4czw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training Loop"
      ],
      "metadata": {
        "id": "P126Y7Zh4czx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "train_model(model, train_data, val_data, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOLqB3gR4czx",
        "outputId": "d496fff0-7c68-43b6-a57d-12f20dc6bdfc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "step 0: train loss 4.6322, val loss 4.6264\n",
            "step 500: train loss 2.1042, val loss 2.1556\n",
            "step 1000: train loss 1.7842, val loss 1.8632\n",
            "step 1500: train loss 1.5551, val loss 1.6657\n",
            "step 2000: train loss 1.4368, val loss 1.5682\n",
            "step 2500: train loss 1.3641, val loss 1.5077\n",
            "step 2999: train loss 1.3166, val loss 1.4706\n",
            "Training finished in 2.27 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Final Evaluation & Text Generation"
      ],
      "metadata": {
        "id": "PSZxTI2O4czx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "idx_path = 'best_model.pt'\n",
        "if os.path.exists(idx_path):\n",
        "    print(\"Loading best model...\")\n",
        "    model.load_state_dict(torch.load(idx_path))\n",
        "else:\n",
        "    print(\"Warning: 'best_model.pt' not found. Using current model state.\")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "evaluate_model(model, test_data, subset_name=\"test\")\n",
        "\n",
        "# Generate Samples\n",
        "print(\"\\n--- Generating Samples ---\")\n",
        "start_strings = [\"Nnipa\", \"Mmaa\", \"Baabi a\", \"Adwuma\", \"Efie\"]\n",
        "\n",
        "for start in start_strings:\n",
        "    print(f\"\\nPrompt: {start}\")\n",
        "    try:\n",
        "        print(generate_text(model, decode, start))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate for {start}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeND4tvc4czx",
        "outputId": "27d72c2c-0280-4005-bd29-0866edf09c79"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model...\n",
            "--- TEST SET EVALUATION ---\n",
            "Loss:       1.4161\n",
            "Perplexity: 4.1210\n",
            "\n",
            "--- Generating Samples ---\n",
            "\n",
            "Prompt: Nnipa\n",
            "Nnipa bebree na bi so gyina hɔ a ɔredi wɔn akadaa.\n",
            "\n",
            "Mmarima mmeranteɛ bi a wɔasorow a ɔrehwɛ wɔn. Mmaa no abɔ baako nso gyina nkyɛn wɔ a wɔresa. Mmarima no ara. Nkurɔfoɔ dan wɔ mu. Ebinom nso wɔ hɔ a ɔkant\n",
            "\n",
            "Prompt: Mmaa\n",
            "Mmaa no anim.\n",
            "\n",
            "Mmarima bi te pono wɔ te hɔ ahyia wɔn so so, na wɔrehwɛ adwuma no yɛ kɔkɔɔ hɔnom. Ebi kyɛ kyɛw. Na ebinom so adeɛ no wɔ hɔnom. Ebi nso yɛ fitaa, wɔde ɛdan no yɛ apadɔtɔ ne mu mmarima nso at\n",
            "\n",
            "Prompt: Baabi a\n",
            "Baabi a ɔkurafoɔ de akɔkɔɔ dɛ obi. Nnipa ketewa no so te so.\n",
            "\n",
            "Nnipadɔm gyinagyina ne ahyia wɔ ne mu a wɔhyɛ ntam. Nkyɛn. Sipa na mmienu wɔn a wɔrekyerɛ egugu adeɛ n’ano a ɔne no ho no ases no mu, ɛna fufuo b\n",
            "\n",
            "Prompt: Adwuma\n",
            "Adwumayɛfoɔ baako a ɔrehwɛ regyerɛ ase. Lɔɔreyɛ soro no soro kame ho nso a wɔreda hɔn nnua na si so wɔn atadeɛ bi nso so.\n",
            "\n",
            "Nkokuo tentena wɔafadaa no so yɛ tadeɛ. Abanin de ahosuo yɛ kuntum. Baako abɔ bi te\n",
            "\n",
            "Prompt: Efie\n",
            "Efie ahodoɔ no so.\n",
            "\n",
            "Maame bi a wɔregu pampataboa a ɛso kwan bi regu na wɔn gyina abɛree tete a maame no mu. Wɔn nyinaa akyi no te tita no anim. Baako nso ɛte soro soro ne ekwan so wɔ hɔ. Atantene boako ns\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}