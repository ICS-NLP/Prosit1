{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "kI_lambd = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper funtions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(input_, vocab):\n",
    "    tokens = input_.lower().split()\n",
    "    return [w if w in vocab else \"<UNK>\" for w in tokens]\n",
    "    \n",
    "def remove_punctuation(text):\n",
    "    # Create a string of all punctuation EXCEPT the single quote\n",
    "    punct_to_remove = string.punctuation.replace(\"'\", \"\")   \n",
    "    return text.translate(str.maketrans('', '', punct_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambdas_trigram(w1, w2, unigram_counts, bigram_counts):\n",
    "    # Context for a trigram is the word pair (w1, w2)\n",
    "    c_w1_w2 = bigram_counts.get((w1, w2), 0)\n",
    "    c_w2 = unigram_counts.get(w2, 0)\n",
    "    \n",
    "    # l3 = Trust in Trigram\n",
    "    l3 = c_w1_w2 / (c_w1_w2 + kI_lambd) if c_w1_w2 > 0 else 0\n",
    "    \n",
    "    # Remaining trust is split between Bigram and Unigram based on Bigram context\n",
    "    remaining = 1 - l3\n",
    "    l2_ratio = c_w2 / (c_w2 + kI_lambd) if c_w2 > 0 else 0\n",
    "    \n",
    "    l2 = remaining * l2_ratio\n",
    "    l1 = remaining * (1 - l2_ratio)\n",
    "    \n",
    "    return l1, l2, l3\n",
    "    \n",
    "\n",
    "\n",
    "def get_lambdas_bigram(w1, unigram_counts):\n",
    "    # Context for a bigram is just the previous word (w1)\n",
    "    context_count = unigram_counts.get(w1, 0)\n",
    "    \n",
    "    # l2 = Trust in Bigram expert\n",
    "    l2 = context_count / (context_count + kI_lambd) if context_count > 0 else 0\n",
    "    \n",
    "    # l1 = Trust in Unigram expert (the leftover)\n",
    "    l1 = 1 - l2\n",
    "    \n",
    "    return l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and clean sentences\n",
    "sentences = []\n",
    "vocab = set()\n",
    "\n",
    "bigram_counts = {}\n",
    "trigram_counts = {}\n",
    "fourgram_counts ={}\n",
    "unigram_counts = {}\n",
    "vocab_size = 0\n",
    "TRAIN = False\n",
    "\n",
    "\n",
    "with open(\"train-transcription-data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = remove_punctuation(line)\n",
    "        if line:\n",
    "            sentences.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence\n",
      "0  Adwumayɛfoɔ nsia adwumayɛfoɔ nson a wɔreyɛ adw...\n",
      "1  Baabi a yɛbu fangoo na ɛhɔ ahye ama nwisie ɛfi...\n",
      "2  Atoyerɛnkyɛm asi wɔ bea a wɔgu fangoo ɛgu ɛhyɛ...\n",
      "3  Mmmaa mmienu ɛne sukuu nkwadaa mmiɛnsa ɛgyina ...\n",
      "4  Nnipa nnum koto hɔ a nhwiren sisi wɔn anim Wɔa...\n",
      "Train size: 15029\n",
      "Dev size: 1879\n",
      "Test size: 1879\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
    "print(df.head())\n",
    "\n",
    "# Split into training and validation sets use dev set as held-out set\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42) \n",
    "    # Split temp into dev and test\n",
    "    # 50% of 20% = 10% each\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Dev size: {len(dev_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n",
    "dev_df.to_csv(\"data/dev.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heldout_corpus(dev_df):\n",
    "    total_tokens = 0\n",
    "    bigram_counts = {}\n",
    "    trigram_counts = {}\n",
    "    fourgram_counts ={}\n",
    "    unigram_counts = {}\n",
    "    \n",
    "\n",
    "    for sentence in dev_df[\"sentence\"]:\n",
    "\n",
    "        \n",
    "        raw_tokens = sentence.lower().split()  \n",
    "        vocab.update(raw_tokens)\n",
    "        for token in raw_tokens:\n",
    "                unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
    "                total_tokens += 1\n",
    "\n",
    "        bigram_tokens = ['<s>'] + raw_tokens + ['</s>']\n",
    "      \n",
    "        for i in range(len(bigram_tokens) - 1):\n",
    "            bigram = (bigram_tokens[i], bigram_tokens[i + 1])\n",
    "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
    "\n",
    "        trigram_tokens = ['<s>', '<s>'] + raw_tokens + ['</s>']\n",
    "        for i in range(len(trigram_tokens) - 2):\n",
    "            trigram = (trigram_tokens[i], trigram_tokens[i + 1], trigram_tokens[i + 2])\n",
    "            trigram_counts[trigram] = trigram_counts.get(trigram, 0) + 1\n",
    "\n",
    "    unigram_df = pd.DataFrame(\n",
    "        [(w1, count) for (w1), count in unigram_counts.items()],\n",
    "        columns=[\"word\", \"count\"]\n",
    "    )\n",
    "\n",
    "    bigram_df = pd.DataFrame(\n",
    "        [(w1, w2, count) for (w1, w2), count in bigram_counts.items()],\n",
    "        columns=[\"word_1\", \"word_2\", \"count\"]\n",
    "    )\n",
    "\n",
    "    trigram_df = pd.DataFrame(\n",
    "        [(w1, w2, w3, count) for (w1, w2, w3), count in trigram_counts.items()],\n",
    "        columns=[\"word_1\", \"word_2\", \"word_3\", \"count\"]\n",
    "    )        \n",
    "\n",
    "    unigram_df.to_csv(\"data/heldout_unigram_counts.csv\")\n",
    "    bigram_df = bigram_df.sort_values(by=\"count\", ascending=False)\n",
    "    bigram_df.to_csv(\"data/heldout_bigram_counts.csv\", index=False)\n",
    " \n",
    "    trigram_df = trigram_df.sort_values(by=\"count\", ascending=False)\n",
    "    trigram_df.to_csv(\"data/heldout_trigram_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "heldout_corpus(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grams(train_df, bigram_counts, trigram_counts, fourgram_counts):\n",
    "    total_tokens = 0\n",
    "    for sentence in train_df[\"sentence\"]:\n",
    "\n",
    "        \n",
    "        raw_tokens = sentence.lower().split()  \n",
    "        vocab.update(raw_tokens)\n",
    "        for token in raw_tokens:\n",
    "                unigram_counts[token] = unigram_counts.get(token, 0) + 1\n",
    "                total_tokens += 1\n",
    "\n",
    "        bigram_tokens = ['<s>'] + raw_tokens + ['</s>']\n",
    "      \n",
    "        for i in range(len(bigram_tokens) - 1):\n",
    "            bigram = (bigram_tokens[i], bigram_tokens[i + 1])\n",
    "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
    "\n",
    "        trigram_tokens = ['<s>', '<s>'] + raw_tokens + ['</s>']\n",
    "        for i in range(len(trigram_tokens) - 2):\n",
    "            trigram = (trigram_tokens[i], trigram_tokens[i + 1], trigram_tokens[i + 2])\n",
    "            trigram_counts[trigram] = trigram_counts.get(trigram, 0) + 1\n",
    "\n",
    "        fourgram_tokens = ['<s>', '<s>', '<s>'] + raw_tokens + ['</s>']\n",
    "        for i in range(len(fourgram_tokens) - 3):\n",
    "            fourgram = (fourgram_tokens[i], fourgram_tokens[i + 1], fourgram_tokens[i + 2], fourgram_tokens[i + 3])\n",
    "            fourgram_counts[fourgram] = fourgram_counts.get(fourgram, 0) + 1\n",
    "\n",
    "\n",
    "    vocab_df = pd.DataFrame(list(vocab),columns=[\"word\"])\n",
    "    unigram_df = pd.DataFrame(\n",
    "        [(w1, count) for (w1), count in unigram_counts.items()],\n",
    "        columns=[\"word\", \"count\"]\n",
    "    )\n",
    "    bigram_df = pd.DataFrame(\n",
    "        [(w1, w2, count) for (w1, w2), count in bigram_counts.items()],\n",
    "        columns=[\"word_1\", \"word_2\", \"count\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    trigram_df = pd.DataFrame(\n",
    "        [(w1, w2, w3, count) for (w1, w2, w3), count in trigram_counts.items()],\n",
    "        columns=[\"word_1\", \"word_2\", \"word_3\", \"count\"]\n",
    "    )\n",
    "    fourgram_df = pd.DataFrame(\n",
    "        [(w1, w2, w3, w4, count) for (w1, w2, w3, w4), count in fourgram_counts.items()],\n",
    "        columns=[\"word_1\", \"word_2\", \"word_3\", \"word_4\", \"count\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    bigram_df = bigram_df.sort_values(by=\"count\", ascending=False)\n",
    "    fourgram_df = fourgram_df.sort_values(by=\"count\", ascending=False)\n",
    "    trigram_df = trigram_df.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    vocab_df.to_csv(\"data/vocab.csv\")\n",
    "    unigram_df.to_csv(\"data/unigram_counts.csv\")\n",
    "    bigram_df.to_csv(\"data/bigram_counts.csv\", index=False)\n",
    "    trigram_df.to_csv(\"data/trigram_counts.csv\", index=False)\n",
    "    fourgram_df.to_csv(\"data/fourgram_counts.csv\", index=False)\n",
    "\n",
    "    print(\"Bigram and trigram counts saved to 'data' directory.\")\n",
    "    print(f\"\"\"  \n",
    "        Vocab size: {len(vocab)}\n",
    "        Number of bigrams: {len(bigram_counts)}\n",
    "        Number of trigrams: {len(trigram_counts)}\n",
    "        Number of 4_grams: {len(fourgram_counts)}\n",
    "    \"\"\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(TRAIN):\n",
    "    vocab = train_grams(train_df, bigram_counts, trigram_counts, fourgram_counts)\n",
    "\n",
    "vocab = pd.read_csv(\"data/vocab.csv\")\n",
    "bigram_df = pd.read_csv(\"data/bigram_counts.csv\")\n",
    "trigram_df = pd.read_csv(\"data/trigram_counts.csv\")\n",
    "hO_unigram_df = pd.read_csv(\"data/heldout_unigram_counts.csv\")\n",
    "hO_bigram_df = pd.read_csv(\"data/heldout_bigram_counts.csv\")\n",
    "hO_trigram_df = pd.read_csv(\"data/heldout_trigram_counts.csv\")\n",
    "fourgram_df = pd.read_csv(\"data/fourgram_counts.csv\")\n",
    "fourgram_pruned = fourgram_df[~fourgram_df['count']<=1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20372"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bi</td>\n",
       "      <td>nso</td>\n",
       "      <td>2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hɔ</td>\n",
       "      <td>a</td>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>mu</td>\n",
       "      <td>2132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bi</td>\n",
       "      <td>a</td>\n",
       "      <td>1846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>so</td>\n",
       "      <td>1627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word_1 word_2  count\n",
       "0     bi    nso   2632\n",
       "1     hɔ      a   2234\n",
       "2     no     mu   2132\n",
       "3     bi      a   1846\n",
       "4     no     so   1627"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>nnipa</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gyina</td>\n",
       "      <td>hɔ</td>\n",
       "      <td>a</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>maame</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>nkurɔfoɔ</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>mmarima</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word_1 word_2    word_3  count\n",
       "0    <s>    <s>     nnipa   1287\n",
       "1  gyina     hɔ         a    566\n",
       "2    <s>    <s>     maame    515\n",
       "3    <s>    <s>  nkurɔfoɔ    432\n",
       "4    <s>    <s>   mmarima    402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "display(vocab_size)\n",
    "display(bigram_df.head())\n",
    "display(trigram_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Smoothing Functions\n",
    "## Bigram, Trigram, 4gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing_bigram(input_, unigram_counts, bigram_counts, vocab):\n",
    "    tokenized_input = input_.lower().split()\n",
    "    last_word = tokenized_input[-1]\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    vocab_probabilities = {}\n",
    "\n",
    "    unigram_count = unigram_counts.get(last_word, 0)\n",
    "\n",
    "    for vocab_word in vocab:\n",
    "        bigram = (last_word, vocab_word)\n",
    "        bigram_count = bigram_counts.get(bigram, 0)\n",
    "\n",
    "        probability = (bigram_count + 1) / (unigram_count + vocab_size)\n",
    "        vocab_probabilities[vocab_word] = probability\n",
    "\n",
    "    top_suggestions = sorted(\n",
    "        vocab_probabilities.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:3]\n",
    "\n",
    "    return top_suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes sentence as input and suggests possible words that comes after the sentence  \n",
    "def laplace_smoothing_trigram(input_, bigram_counts, trigram_counts, vocab):\n",
    "    # Consider the last bigram of sentence\n",
    "    tokenized_input = input_.lower().split()\n",
    "    last_bigram = tuple(tokenized_input[-2:])\n",
    "    \n",
    "    # Calculating probability for each word in vocab\n",
    "    vocab_probabilities = {}\n",
    "\n",
    "    # Laplace Smoothing\n",
    "    for vocab_word in vocab:\n",
    "        trigram = (last_bigram[0], last_bigram[1], vocab_word)\n",
    "\n",
    "        trigram_count = trigram_counts.get(trigram, 0)\n",
    "        bigram_count = bigram_counts.get(last_bigram, 0)\n",
    "        \n",
    "         # Laplace smoothing formula\n",
    "        probability = (trigram_count + 1) / (bigram_count + vocab_size)\n",
    "        vocab_probabilities[vocab_word] = probability\n",
    "    \n",
    "    # Sorting the vocab probability in descending order to get top probable words\n",
    "    top_suggestions = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    return top_suggestions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-k Smoothing Functions\n",
    "## Bigram, Trigram, 4-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_k_smoothing_trigram(input_, bigram_counts, trigram_counts, vocab, k=0.1):\n",
    "    tokenized_input = input_.lower().split()\n",
    "    last_bigram = tuple(tokenized_input[-2:])\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    vocab_probabilities = {}\n",
    "\n",
    "    bigram_count = bigram_counts.get(last_bigram, 0)\n",
    "\n",
    "    for vocab_word in vocab:\n",
    "        trigram = (last_bigram[0], last_bigram[1], vocab_word)\n",
    "        trigram_count = trigram_counts.get(trigram, 0)\n",
    "\n",
    "        probability = (trigram_count + k) / (bigram_count + k * vocab_size)\n",
    "        vocab_probabilities[vocab_word] = probability\n",
    "\n",
    "    top_suggestions = sorted(\n",
    "        vocab_probabilities.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:3]\n",
    "\n",
    "    return top_suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_add_k_prob(word, context, counts, context_counts, vocab_size, k=0.1):\n",
    "    \"\"\"\n",
    "    Standard Add-K probability for any N-gram level.\n",
    "    counts: dict of (context + word)\n",
    "    context_counts: dict of (context)\n",
    "    \"\"\"\n",
    "    count_ngram = counts.get(context + (word,), 0)\n",
    "    count_context = context_counts.get(context, 0)\n",
    "    return (count_ngram + k) / (count_context + k * vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation\n",
    "## Bigram, Trigram\n",
    "\n",
    "### P(w2​∣w1​) = λ P_bigram​(w2​∣w1​) + (1−λ)P_unigram​(w2​)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_interpolated_trigram_prob(w3, w1, w2, unigram_counts, bigram_counts, trigram_counts, total_tokens):\n",
    "    \"\"\"\n",
    "    Calculates P(w3 | w1, w2) using interpolated weights.\n",
    "    \"\"\"\n",
    "    # Get dynamic lambdas based on context\n",
    "    l1, l2, l3 = get_lambdas_trigram(w1, w2, unigram_counts, bigram_counts)\n",
    "    \n",
    "    # MLE Probabilities\n",
    "    p3 = trigram_counts.get((w1, w2, w3), 0) / bigram_counts.get((w1, w2), 1) if (w1, w2) in bigram_counts else 0\n",
    "    p2 = bigram_counts.get((w2, w3), 0) / unigram_counts.get(w2, 1) if w2 in unigram_counts else 0\n",
    "    p1 = unigram_counts.get(w3, 0) / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    return (l3 * p3) + (l2 * p2) + (l1 * p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(test_df, prob_func, **kwargs):\n",
    "    \"\"\"\n",
    "    test_df: DataFrame containing sentences\n",
    "    prob_func: The probability logic to use\n",
    "    **kwargs: Counts, vocab, and k-values needed by the prob_func\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_word_count = 0\n",
    "    \n",
    "    for sentence in test_df[\"sentence\"]:\n",
    "        # Preprocess and pad\n",
    "        tokens = [\"<s>\", \"<s>\"] + sentence.lower().split() + [\"</s>\"]\n",
    "        \n",
    "        for i in range(2, len(tokens)):\n",
    "            w1, w2, w3 = tokens[i-2], tokens[i-1], tokens[i]\n",
    "            \n",
    "            # Call the passed probability function\n",
    "            prob = prob_func(w3, w1, w2, **kwargs)\n",
    "            \n",
    "            # Use a tiny floor for probability to avoid log(0) if smoothing fails\n",
    "            prob = max(prob, 1e-10) \n",
    "            \n",
    "            total_log_prob += math.log2(prob)\n",
    "            total_word_count += 1\n",
    "            \n",
    "    avg_log_prob = total_log_prob / total_word_count\n",
    "    return math.pow(2, -avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add-K Perplexity: 20372.000000278156\n"
     ]
    }
   ],
   "source": [
    "# Wrapper to match the perplexity function signature\n",
    "def add_k_wrapper(w3, w1, w2, **kwargs):\n",
    "    return get_add_k_prob(w3, (w1, w2), kwargs['trigram_counts'], kwargs['bigram_counts'], kwargs['vocab_size'], k=0.5)\n",
    "\n",
    "pp_add_k = calculate_perplexity(test_df, add_k_wrapper, \n",
    "                                trigram_counts=trigram_counts, \n",
    "                                bigram_counts=bigram_counts, \n",
    "                                vocab_size=len(vocab))\n",
    "print(f\"Add-K Perplexity: {pp_add_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me pɛ sɛ me word word word word word word word word word word word word word word word word word word word word\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(primer, vocab, unigram_counts, bigram_counts, trigram_counts, total_tokens, max_len=20):\n",
    "    \"\"\"\n",
    "    Generates a sentence token by token using interpolated trigram probabilities.\n",
    "    \"\"\"\n",
    "    # 1. Prepare the sequence from your primer\n",
    "    # \"me pɛ sɛ me\" -> ['me', 'pɛ', 'sɛ', 'me']\n",
    "    sentence = primer.lower().split()\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # 2. Identify the context (the last two words)\n",
    "        w1 = sentence[-2] if len(sentence) >= 2 else None\n",
    "        w2 = sentence[-1]\n",
    "        \n",
    "        best_word = None\n",
    "        max_prob = -1\n",
    "        \n",
    "        # 3. Efficiency Trick: Only check words that have a chance of appearing\n",
    "        # We look at words that appeared after w2 in the training data\n",
    "        candidates = [word for (prev, word) in bigram_counts.keys() if prev == w2]\n",
    "        \n",
    "        # If w2 is totally new, fallback to a small subset of the vocab or unigrams\n",
    "        if not candidates:\n",
    "            candidates = list(vocab)[:500] \n",
    "\n",
    "        for candidate in set(candidates):\n",
    "            # Calculate P(candidate | w1, w2) using your interpolation function\n",
    "            prob = get_interpolated_trigram_prob(\n",
    "                candidate, w1, w2, \n",
    "                unigram_counts, bigram_counts, trigram_counts, total_tokens\n",
    "            )\n",
    "            \n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                best_word = candidate\n",
    "        \n",
    "        # 4. Append and check for sentence end\n",
    "        if best_word is None: break\n",
    "        sentence.append(best_word)\n",
    "        \n",
    "        if best_word in {'.', '!', '?'}:\n",
    "            break\n",
    "            \n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# Execution\n",
    "# Ensure unigram_counts is defined as: Counter(lower_case_corpus)\n",
    "total_tokens = sum(unigram_counts.values())\n",
    "output = generate_sentence(\"me pɛ sɛ me\", vocab, unigram_counts, bigram_counts, trigram_counts, total_tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/patrickadu-amankwah/nltk_data'\n    - '/Users/patrickadu-amankwah/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/nltk_data'\n    - '/Users/patrickadu-amankwah/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/share/nltk_data'\n    - '/Users/patrickadu-amankwah/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mme pɛ sɛ me\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munigram_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigram_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigram_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munigram_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mgenerate_sentence\u001b[39m\u001b[34m(primer, vocab, unigram_counts, bigram_counts, trigram_counts, total_tokens, max_len)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Start with the tokens from your primer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m generated_sentence = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 1. Get the current context (last two words)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# If the sentence is too short, we pad it or handle it as a bigram\u001b[39;00m\n\u001b[32m     14\u001b[39m     w1 = generated_sentence[-\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(generated_sentence) >= \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/patrickadu-amankwah/nltk_data'\n    - '/Users/patrickadu-amankwah/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/nltk_data'\n    - '/Users/patrickadu-amankwah/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/share/nltk_data'\n    - '/Users/patrickadu-amankwah/Documents/Msc Intelligent Computing Systems/NLP/n_gram_twi_model/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "generate_sentence(\"me pɛ sɛ me\", vocab, unigram_counts, bigram_counts, trigram_counts, sum(unigram_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Tweaks\n",
    "\n",
    "* Adding corpus (nltk, scrapping, etc)\n",
    "* DIfferent model better than trigram model\n",
    "* Handling 0 counts in model (Smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
